{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# First get the auth set up.\n",
    "\n",
    "import os\n",
    "from scholarly import scholarly, ProxyGenerator\n",
    "\n",
    "SCRAPER_API_KEY = os.getenv('SCRAPER_API_KEY')\n",
    "\n",
    "pg = ProxyGenerator()\n",
    "success = pg.ScraperAPI(SCRAPER_API_KEY)\n",
    "\n",
    "print(success)\n",
    "\n",
    "scholarly.use_proxy(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social AND \"hierarchy i mean\" -\"analytic hierarchy process\" -\"analytical hierarchy process\" -\"response hierarchy\" -\"polynomial-time hierarchy\" -\"polynomial hierarchy\" -\"gauge hierarchy\" -\"geometric hierarchy\" -\"hierarchy of needs\" -\"hierarchy of effects\" -\"boundary hierarchy\" -\"toda hierarchy\" -\"mass hierarchy\" -\"hamiltonian hierarchy\" -\"hierarchy of effects\" -\"fuzzy hierarchy\" -\"semantic hierarchy\" -\"image hierarchy\" -\"cognitive hierarchy\"\n",
      "Core search name (for files): hierarchy_i_mean\n"
     ]
    }
   ],
   "source": [
    "# Setting search queries\n",
    "\n",
    "# This is the only place where the search term needs to be assigned.\n",
    "# We'll use the core_search_term later to build the filename\n",
    "# If dates are used, the \"high\" (ending) date shoudl be included\n",
    "core_search_term = '\"hierarchy i mean\"' \n",
    "\n",
    "core_search_name = core_search_term.replace(\" \", \"_\")\n",
    "core_search_name = core_search_name.replace('\"', \"\")\n",
    "# core_search_name = core_search_name + '2007'\n",
    "# core_search_name = core_search_name + '2009'\n",
    "# core_search_name = core_search_name + '2015'\n",
    "# core_search_name = core_search_name + '2019'\n",
    "# core_search_name = core_search_name + '2022'\n",
    "\n",
    "# The params for the search\n",
    "# search_pubs(self,\n",
    "#             query: str, \n",
    "#             patents: bool = True,\n",
    "#             citations: bool = True,\n",
    "#             delay_from_function: any = None)\n",
    "\n",
    "# searchstr = 'climate change retractions pubpeer'\n",
    "search_str = 'social AND ' + core_search_term + '''\n",
    " -\"analytic hierarchy process\"\n",
    " -\"analytical hierarchy process\"\n",
    " -\"response hierarchy\"\n",
    " -\"polynomial-time hierarchy\"\n",
    " -\"polynomial hierarchy\"\n",
    " -\"gauge hierarchy\"\n",
    " -\"geometric hierarchy\"\n",
    " -\"hierarchy of needs\"\n",
    " -\"hierarchy of effects\"\n",
    " -\"boundary hierarchy\"\n",
    " -\"toda hierarchy\"\n",
    " -\"mass hierarchy\"\n",
    " -\"hamiltonian hierarchy\"\n",
    " -\"hierarchy of effects\"\n",
    " -\"fuzzy hierarchy\"\n",
    " -\"semantic hierarchy\"\n",
    " -\"image hierarchy\"\n",
    " -\"cognitive hierarchy\"'''\n",
    "\n",
    "search_str = search_str.replace(\"\\n\", \"\")\n",
    "print(search_str)\n",
    "print('Core search name (for files): ' + core_search_name)\n",
    "\n",
    "# search_query = scholarly.search_pubs('covid-19 retractions pubpeer') # '\"test query\"+\"cast iron\"'\n",
    "# search_query = scholarly.search_pubs('climate change retractions pubpeer') # 389 results, via scraper\n",
    "# search_query = scholarly.search_pubs('\"when coalitional cues\" AND social') # ~7 items\n",
    "# search_query = scholarly.search_pubs('\"hyperemitting power plants\"') #  items\n",
    "# search_query = scholarly.search_pubs('\"universal state phenomenon\"')\n",
    "# search_query = scholarly.search_pubs('\"This type of graph can be used to compare organizations\"')\n",
    "# search_query = scholarly.search_pubs('\"Max Weber suggested\"')\n",
    "# search_query = scholarly.search_pubs('\"shoes worn thin\"') # ~ 17\n",
    "# search_query = scholarly.search_pubs('\"boyhood in the bronx\"') # ~ ??? (170 or so?) But it gave 33\n",
    "# search_query = scholarly.search_pubs('\"duckduckgo\"') ~ 5170\n",
    "# search_query = scholarly.search_pubs('\"body that precedes it\"') # ~ ???\n",
    "# search_query = scholarly.search_pubs('\"major developments in political science in the past decade\"') # only 1???\n",
    "# search_query = scholarly.search_pubs('\"exemplary of a dynamic that\"') # ?? < 13 results\n",
    "# search_query = scholarly.search_pubs('\"externalities are effects on the ecosystem or on other parties\"') # ?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query with scholarly\n",
    "\n",
    "# Includes function to delay between page views\n",
    "# because the query goes through all of them asap.\n",
    "\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "def generate_page_time():\n",
    "    result = np.random.gamma(4.5, 1.25, 1)\n",
    "    result = result.__add__(2.5)\n",
    "    result = np.round_(result, decimals = 3)\n",
    "    result = result[0]\n",
    "    return result\n",
    "\n",
    "search_query = scholarly.search_pubs(query=search_str, \n",
    "                                     patents=False,\n",
    "                                     citations=False,\n",
    "                                     # delay_from_function=generate_page_time() # commentout to disable\n",
    "                                     # year_low: int = None,\n",
    "                                     # year_high: int = None,\n",
    "\n",
    "                                    #  year_low=None,\n",
    "                                    #  year_high=2009,\n",
    "                                    \n",
    "                                    #  year_low=2010,\n",
    "                                    #  year_high=2015,\n",
    "\n",
    "                                    #  year_low=2016,\n",
    "                                    #  year_high=2019,\n",
    "\n",
    "                                    #  year_low=2020,\n",
    "                                    #  year_high=2022,\n",
    "                                    )\n",
    "\n",
    "# When using dates, the quartiles split roughly into these years, so 25% of results will be in each bin.\n",
    "# min 2009\n",
    "# 2010 2015\n",
    "# 2016 2019\n",
    "# 2020 2022\n",
    "# These were based on me doing a quartile analysis of a 900+ result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results number given by GS: 197\n"
     ]
    }
   ],
   "source": [
    "# Print the total results.\n",
    "# Google search is almost always wrong about this number,\n",
    "# so the array will likely be of different length.\n",
    "\n",
    "print('Total results number given by GS: ' + str(search_query.total_results) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to write a json file WITHOUT fill\n",
    "# from the array we build from going through the results\n",
    "# of the search query.\n",
    "\n",
    "# The difference between fill and WITHOUT fill is \n",
    "# whether I call the bibtex() or not.\n",
    "\n",
    "# When the publication entries are not filled,\n",
    "# I can use other scripts to build the bib files.\n",
    "# And I do this in another file, bib_maker.ipynb\n",
    "\n",
    "import ujson\n",
    "# import json\n",
    "# import time\n",
    "\n",
    "num_ttl_results = search_query.total_results # unlikely to precisely match len of array\n",
    "# print('Total results number given by GS: ' + str(num_ttl_results) )\n",
    "\n",
    "pub_array = []\n",
    "pub_counter = 0\n",
    "json_counter = 0 \n",
    "\n",
    "for item in search_query:\n",
    "    # print('Attempting to append pub_array, index ' + str(pub_counter))\n",
    "    pub_array.append(item)\n",
    "    # pub_counter = pub_counter + 1\n",
    "\n",
    "    # old code, I used to do both in the loop, faster to do separate\n",
    "    # with open(str(num_ttl_results) + core_search_name + '_pub_without_fill.json', 'a') as filehandle:\n",
    "    #     ujson.dump(item, filehandle)     \n",
    "    #     # print(str(json_counter) + 'th JSON entry appended')\n",
    "    #     # json_counter = json_counter + 1\n",
    "\n",
    "# The .append takes most of the time. Python append to list is pretty optimized.\n",
    "# There may not be a faster way to do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(num_ttl_results) + core_search_name + '_pub_without_fill.json', 'a') as filehandle:\n",
    "    for thing in pub_array:\n",
    "        ujson.dump(thing, filehandle) \n",
    "\n",
    "# ujson 5.4.0 makes the json part much faster. orjson would make it even faster, but with coding work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "jsonfilename = '197hierarchy_i_mean_pub_without_fill.json'\n"
     ]
    }
   ],
   "source": [
    "print(len(pub_array))\n",
    "name_for_bib_maker_script = str(num_ttl_results) + core_search_name + '_pub_without_fill.json'\n",
    "print(\"jsonfilename = '\" + name_for_bib_maker_script + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(search_query._rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code below is ONLY to fill the metadata, will take 26 credits per publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to write a json WITH fill.\n",
    "# Build the data I need from the query results, write it piece by piece to a file.\n",
    "\n",
    "# I can write the not-filled iterator to a file, \n",
    "# and come back and run a fill on each item and write those to a file.\n",
    "# If I run bibtex on each item in the iterator, I will get a bib entry back\n",
    "# that will NOT contain a bunch of the filled metadata.\n",
    "\n",
    "import numpy as np \n",
    "import time\n",
    "import json\n",
    "\n",
    "def generate_page_time():\n",
    "    result = np.random.gamma(4.5, 1.25, 1)\n",
    "    result = result.__add__(2.5)\n",
    "    result = np.round_(result, decimals = 3)\n",
    "    result = result[0]\n",
    "    return result\n",
    "\n",
    "# Set up counter and empty stuff\n",
    "\n",
    "fill_array = []\n",
    "json_counter = 0 \n",
    "\n",
    "num_ttl_results = search_query.total_results\n",
    "print('Total results number given by GS: ' + str(num_ttl_results) )\n",
    "\n",
    "fill_time = time.time()\n",
    "\n",
    "for item in search_query:\n",
    "    print('Trying to fill and append item. fill_array is ' + str(len(fill_array)) + ' items long.')\n",
    "    fill_array.append(scholarly.fill(item)) # bibtex() calls .fill() to get full metadata\n",
    "    time.sleep(generate_page_time()) # If I want it to sleep between fills\n",
    "\n",
    "print (\"Fill time took \", time.time() - fill_time, \" seconds.\")\n",
    "\n",
    "json_time = time.time()\n",
    "\n",
    "with open(str(num_ttl_results) + core_search_name + '_filled_bib.json', 'a') as filehandle:\n",
    "    for fill_item in fill_array:\n",
    "        json.dump(fill_item, filehandle)     \n",
    "        print(str(json_counter) + 'th JSON entry appended')\n",
    "        json_counter = json_counter + 1\n",
    "\n",
    "print('ttl fill_array length: ' + str(len(fill_array)))\n",
    "print (\"Writing json file took \", time.time() - json_time, \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing code for the delay because I really don't think it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I haven't figured out how to do this yet.\n",
    "class fooiterator(object):\n",
    "    def __init__(self):\n",
    "        self.someproperty = \"123\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "812f74ffa169d7186919aad1b064e1de01ac83f7d294665c43dd347688936873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
